## 训练：
直接求解导数方程的方式在多数情况下较困难，本质原因是导数方程往往正向求解容易（已知X，求得Y），反向求解较难（已知Y，求得X）。这种特性的方程在很多加密算法中较为常见，与日常见到的锁头特性一样：已知“钥匙”，锁头判断是否正确容易；已知“锁头”，反推钥匙的形状比较难。
这种情况特别类似于一位想从山峰走到坡谷的盲人，他看不见坡谷在哪（无法逆向求解出Loss导数为0时的参数值），但可以伸脚探索身边的坡度（当前点的导数值，也称为梯度）。那么，求解Loss函数最小值可以“从当前的参数取值，一步步的按照下坡的方向下降，直到走到最低点”实现。这种方法个人称它为“瞎子下坡法”。哦不，有个更正式的说法“梯度下降法”。
现在我们要找出一组[w5,w9]的值，使得损失函数最小，实现梯度下降法的方案如下：
- 随机的选一组初始值[w5,w9]
- 选取下一个点[w5′,w9′]， 使得loss减小
- 重复上面的步骤2，直到损失函数几乎不再下降
如何选择[w5′,w9′]是至关重要的，第一要保证LLL是下降的，第二要使得下降的趋势尽可能的快。微积分的基础知识告诉我们，沿着梯度的反方向，是函数值下降最快的方向

在上述程序中，每次迭代的时候均基于数据集中的全部数据进行计算。但在实际问题中数据集往往非常大，如果每次计算都使用全部的数据来计算损失函数和梯度，效率非常低。一个合理的解决方案是每次从总的数据集中随机抽取出小部分数据来代表整体，基于这部分数据计算梯度和损失，然后更新参数。这种方法被称作随机梯度下降法（Stochastic Gradient Descent），简称SGD。每次迭代时抽取出来的一批数据被称为一个min-batch，一个mini-batch所包含的样本数目称为batch_size。当程序迭代的时候，按mini-batch逐渐抽取出样本，当把整个数据集都遍历到了的时候，则完成了一轮的训练，也叫一个epoch。启动训练时，可以将训练的轮数num_epochs和batch_size作为参数传入。

模型假设，评价函数（损失/优化目标）和优化算法是构成一个模型的三个部分。

这种变换已经复杂到无法用数学公式表达，所以研究者们借鉴了人脑神经元的结构，设计出神经网络的模型。

多数情况下，这些实现只是相对有限的一些选择，比如常见的Loss函数不超过十种，常用的网络配置也就十几种，常用优化算法不超过五种等等